{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cfd4cea",
   "metadata": {},
   "source": [
    "## train with huggingface dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b780e07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train on example data[ydshieh/coco_dataset_script], this dataset is hosted by huggingface.\n",
    "\n",
    "import towhee\n",
    "\n",
    "#step1 \n",
    "#get the operator, modality has no effect to the training model, it is only for the inference branch selection.\n",
    "clip_op = towhee.ops.image_text_embedding.clip(model_name='clip_vit_base_patch16', modality='image').get_op()\n",
    "\n",
    "\n",
    "#step2 \n",
    "#trainer configuration, theses parameters are huggingface-style standard training configuration.\n",
    "data_args = {\n",
    "    'dataset_name': 'ydshieh/coco_dataset_script',\n",
    "    'dataset_config_name': '2017',\n",
    "    'cache_dir': './cache',\n",
    "    'max_seq_length': 77,\n",
    "    'data_dir': '/path_to_your_data',\n",
    "    'image_mean': [0.48145466, 0.4578275, 0.40821073],\n",
    "    \"image_std\": [0.26862954, 0.26130258, 0.27577711]\n",
    "}\n",
    "training_args = {\n",
    "    'num_train_epochs': 150, # you can add epoch number to get a better metric.\n",
    "    'per_device_train_batch_size': 8,\n",
    "    'per_device_eval_batch_size': 8,\n",
    "    'do_train': True,\n",
    "    'do_eval': True,\n",
    "    'remove_unused_columns': False,\n",
    "    'dataloader_drop_last': True,\n",
    "    'output_dir': '/path_to_your_save',\n",
    "    'overwrite_output_dir': True,\n",
    "}\n",
    "\n",
    "#step3 \n",
    "#train your model\n",
    "clip_op.train(data_args=data_args, training_args=training_args)\n",
    "\n",
    "\n",
    "#step4 \n",
    "#load your trained checkpoints\n",
    "clip_op = towhee.ops.image_text_embedding.clip(model_name='clip_vit_base_patch16', modality='image', checkpoint_path=\"/tmp/test-clip/checkpoint-6500/pytorch_model.bin\").get_op()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acebe6ce",
   "metadata": {},
   "source": [
    "## train with customized dataset\n",
    "Let's assume there is a dataset like this.\n",
    "\n",
    "|caption ID|image ID | caption   |  image  | image path|\n",
    "|:--------|:-------- |:----------|:--------|:----------|\n",
    "| 0 | 0 | a woman is smiling in the car.|  <img src=\"images/image1.png\" max-width=\"100\" width=\"200\" height=\"200\"/>| /tmp/dataset/image1.png |\n",
    "| 1 | 1 | a kitten and a puppy are sitting on the grass. |  <img src=\"images/image2.png\" max-width=\"100\" width=\"200\" height=\"200\"/>| /tmp/dataset/image2.png  |\n",
    "| 2 | 1 | a cat is watching at a dog on the grass. |  <img src=\"images/image2.png\" max-width=\"100\" width=\"200\" height=\"200\"/>| /tmp/dataset/image2.png  |\n",
    "| 3 | 2 | two kids are playing in the colorful balloons.|  <img src=\"images/image3.png\" max-width=\"100\" width=\"200\" height=\"200\"/>| /tmp/dataset/image3.png  |\n",
    "| 4 | 3 | a tiger is running in the snow.|  <img src=\"images/image4.png\" max-width=\"100\" width=\"200\" height=\"200\"/>| /tmp/dataset/image4.png  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1921f8",
   "metadata": {},
   "source": [
    "we need to perpare a json file to describe this dataset in the format as below.\n",
    "```json\n",
    "[\n",
    "   {\n",
    "      \"caption_id\":0,\n",
    "      \"image_id\":0,\n",
    "      \"caption\":\"a woman is smiling in the car.\",\n",
    "      \"image_path\":\"/tmp/dataset/image1.png\"\n",
    "   },\n",
    "   {\n",
    "      \"caption_id\":1,\n",
    "      \"image_id\":1,\n",
    "      \"caption\":\"a kitten and a puppy are sitting on the grass.\",\n",
    "      \"image_path\":\" /tmp/dataset/image2.png\"\n",
    "   },\n",
    "   {\n",
    "      \"caption_id\":2,\n",
    "      \"image_id\":1,\n",
    "      \"caption\":\"a cat is watching at a dog on the grass.\",\n",
    "      \"image_path\":\"/tmp/dataset/image2.png\"\n",
    "   },\n",
    "   {\n",
    "      \"caption_id\":3,\n",
    "      \"image_id\":2,\n",
    "      \"caption\":\"two kids are playing in the colorful balloons.\",\n",
    "      \"image_path\":\" /tmp/dataset/image3.png\"\n",
    "   },\n",
    "   {\n",
    "      \"caption_id\":4,\n",
    "      \"image_id\":3,\n",
    "      \"caption\":\"a tiger is running in the snow.\",\n",
    "      \"image_path\":\" /tmp/dataset/image4.png\"\n",
    "   }\n",
    "]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2f563e",
   "metadata": {},
   "source": [
    "we can do a sanity-check make sure the dataset is created properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dff5421",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-486f4ab57d32eb72\n",
      "/mnt/disk1/david.wxy/Anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/zilliz/.cache/huggingface/datasets/json/default-486f4ab57d32eb72/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a53adcb98a940bda6012babefd8a7e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f304cf248e4dcfa723c76f582fb21f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7227ff84eb64204a627001316ff0a02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/zilliz/.cache/huggingface/datasets/json/default-486f4ab57d32eb72/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a34926b5d30f4327a90e4ce2e9f2655a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'caption_id': 0, 'image_id': 0, 'caption': 'a woman is smiling in the car.', 'image_path': '/tmp/dataset/image1.png'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_args = {}\n",
    "data_args['train'] = 'traindata_sample.json'\n",
    "\n",
    "extension = 'json'\n",
    "dataset = load_dataset(\n",
    "    extension,\n",
    "    data_files=data_args,\n",
    "    cache_dir=None,\n",
    "    use_auth_token=False\n",
    ")\n",
    "\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74c2b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train on customized dataset is hosted by huggingface.\n",
    "\n",
    "import towhee\n",
    "\n",
    "#step1 \n",
    "#get the operator, modality has no effect to the training model, it is only for the inference branch selection.\n",
    "clip_op = towhee.ops.image_text_embedding.clip(model_name='clip_vit_base_patch16', modality='image').get_op()\n",
    "\n",
    "\n",
    "#step2 \n",
    "#trainer configuration, theses parameters are huggingface-style standard training configuration.\n",
    "data_args = {\n",
    "    'dataset_name': None,\n",
    "    'dataset_config_name': None,\n",
    "    'train_file': 'train_data.json',\n",
    "    'validation_file': 'val_data.json',\n",
    "    'cache_dir': './cache',\n",
    "    'max_seq_length': 77,\n",
    "    'data_dir': 'path_to_your_data',\n",
    "    'image_mean': [0.48145466, 0.4578275, 0.40821073],\n",
    "    \"image_std\": [0.26862954, 0.26130258, 0.27577711]\n",
    "}\n",
    "\n",
    "\n",
    "#step3 \n",
    "#train your model\n",
    "clip_op.train(data_args=data_args, training_args=training_args)\n",
    "\n",
    "\n",
    "#step4 \n",
    "#load your trained checkpoints\n",
    "clip_op = towhee.ops.image_text_embedding.clip(model_name='clip_vit_base_patch16', modality='image', checkpoint_path=\"path_to_your_trained_model\").get_op()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
